{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "TF version is 1.8.0.\n"
     ]
    }
   ],
   "source": [
    "import warnings,os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim # I lkie slim \n",
    "import matplotlib.pyplot as plt\n",
    "# warnings.filterwarnings(\"ignore\") # Stop showing annoying warnings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) # I like old-style MNIST w/o warnings\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.utils import shuffle\n",
    "from util import gpu_sess\n",
    "mnist = input_data.read_data_sets('data', one_hot=True)\n",
    "%matplotlib inline\n",
    "print (\"TF version is %s.\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define vae class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vae_class(object):\n",
    "    def __init__(self,_name='VAE',_xDim=784,_zDim=10,_hDims=[64,64],_cDim=0,\n",
    "                 _actv=tf.nn.relu,_bn=slim.batch_norm,\n",
    "                 _lr=0.001,_beta1=0.9,_beta2=0.9,_epsilon=0.1,\n",
    "                 _VERBOSE=True):\n",
    "        self.name  = _name # Name\n",
    "        self.xDim  = _xDim # Dimension of input\n",
    "        self.zDim  = _zDim # Dimension of latent vector\n",
    "        self.hDims = _hDims # Dimention of hidden layer(s)\n",
    "        self.cDim  = _cDim # Dimention of conditional vector \n",
    "        self.actv  = _actv # Activation function \n",
    "        self.bn    = _bn # Batch norm (slim.batch_norm / None)\n",
    "        self.lr    = _lr # Learning rate \n",
    "        self.beta1 = _beta1 # Adam related (beta1)\n",
    "        self.beta2 = _beta2 # Adam related (beta2)\n",
    "        self.epsilon = _epsilon # Adam related (epsilon)\n",
    "        self.VERBOSE = _VERBOSE\n",
    "        if self.VERBOSE:\n",
    "            print (\"[%s] xdim:[%d] zdim:[%d] hdim:%s cdim:[%d]\"\\\n",
    "                % (self.name,self.xDim,self.zDim,self.hDims,self.cDim))\n",
    "        # Make model \n",
    "        self._build_model()\n",
    "        self._build_graph()\n",
    "        self._check_params()\n",
    "    def _build_model(self):\n",
    "        # Placeholders\n",
    "        self.x  = tf.placeholder(tf.float32, shape=[None,self.xDim], name=\"x\") # This will be inputs\n",
    "        self.z  = tf.placeholder(tf.float32, shape=[None,self.zDim], name=\"z\") # Latent vectors\n",
    "        self.c  = tf.placeholder(tf.float32, shape=[None,self.cDim], name=\"c\") # Conditioning vectors\n",
    "        self.q  = tf.placeholder(tf.float32, shape=[None], name=\"q\") # Weighting vectors \n",
    "        self.kp = tf.placeholder(tf.float32) # Keep prob.\n",
    "        self.klWeight = tf.placeholder(tf.float32) # KL weight heuristics \n",
    "        self.isTraining = tf.placeholder(dtype=tf.bool,shape=[]) # Training flag\n",
    "        # Build graph\n",
    "        self.bnInit = {'beta':tf.constant_initializer(0.),'gamma':tf.constant_initializer(1.)}\n",
    "        self.bnParams = {'is_training':self.isTraining,'decay':0.9,'epsilon':1e-5,\n",
    "                           'param_initializers':self.bnInit,'updates_collections':None}\n",
    "        with tf.variable_scope(self.name,reuse=False) as scope:\n",
    "            with slim.arg_scope([slim.fully_connected],activation_fn=self.actv,\n",
    "                               weights_initializer=tf.random_normal_initializer(stddev=0.1),\n",
    "                               biases_initializer=tf.constant_initializer(value=0.0),\n",
    "                               normalizer_fn=self.bn,normalizer_params=self.bnParams,\n",
    "                               weights_regularizer=None):\n",
    "                _net = self.x\n",
    "                self.N = tf.shape(self.x)[0] # Number of current inputs \n",
    "                # Encoder \n",
    "                for hIdx in range(len(self.hDims)): # Loop over hidden layers\n",
    "                    _hDim = self.hDims[hIdx]\n",
    "                    _net = slim.fully_connected(_net,_hDim,scope='enc_lin'+str(hIdx))\n",
    "                    _net = slim.dropout(_net,keep_prob=self.kp,is_training=self.isTraining\n",
    "                                        ,scope='enc_dr'+str(hIdx))\n",
    "                # Latent vector z (NO ACTIVATION!)\n",
    "                self.zMuEncoded = slim.fully_connected(_net,self.zDim,scope='zMuEncoded',activation_fn=None)\n",
    "                self.zLogVarEncoded = slim.fully_connected(_net,self.zDim,scope='zLogVarEncoded',activation_fn=None)\n",
    "                # Define z sampler (reparametrization trick)\n",
    "                self.eps = tf.random_normal(shape=(self.N,self.zDim),mean=0.,stddev=1.,dtype=tf.float32)\n",
    "                self.zSample = self.zMuEncoded+tf.sqrt(tf.exp(self.zLogVarEncoded))*self.eps\n",
    "                # Concatenate the condition vector to the sampled latent vector\n",
    "                if self.cDim != 0:\n",
    "                    self.zEncoded = tf.concat([self.zSample,self.c],axis=1)\n",
    "                else:\n",
    "                    self.zEncoded = self.zSample\n",
    "                # Decoder \n",
    "                _net = self.zEncoded\n",
    "                for hIdx in range(len(self.hDims)): # Loop over hidden layers\n",
    "                    _hDim = self.hDims[len(self.hDims)-hIdx-1]\n",
    "                    _net = slim.fully_connected(_net,_hDim,scope='dec_lin'+str(hIdx))\n",
    "                    _net = slim.dropout(_net,keep_prob=self.kp,is_training=self.isTraining\n",
    "                                        ,scope='dec_dr'+str(hIdx))\n",
    "                # Reconstruct output (NO ACTIVATION)\n",
    "                self.xRecon = slim.fully_connected(_net,self.xDim,scope='xRecon',activation_fn=None)\n",
    "        # Additional graph for debugging purposes\n",
    "        with tf.variable_scope(self.name,reuse=True) as scope:\n",
    "            with slim.arg_scope([slim.fully_connected],activation_fn=self.actv,\n",
    "                               weights_initializer=tf.random_normal_initializer(stddev=0.1),\n",
    "                               biases_initializer=tf.constant_initializer(value=0.0),\n",
    "                               normalizer_fn=self.bn,normalizer_params=self.bnParams,\n",
    "                               weights_regularizer=None):\n",
    "                # Start from given z, instead of sampled z\n",
    "                if self.cDim != 0:\n",
    "                    self.zGiven = tf.concat([self.z,self.c],axis=1)\n",
    "                else:\n",
    "                    self.zGiven = self.z\n",
    "                # Decoder \n",
    "                _net = self.zGiven\n",
    "                for hIdx in range(len(self.hDims)): # Loop over hidden layers\n",
    "                    _hDim = self.hDims[len(self.hDims)-hIdx-1]\n",
    "                    _net = slim.fully_connected(_net,_hDim,scope='dec_lin'+str(hIdx))\n",
    "                # Reconstruct output (NO ACTIVATION)\n",
    "                self.xGivenZ = slim.fully_connected(_net,self.xDim,scope='xRecon',activation_fn=None)\n",
    "    def _build_graph(self):\n",
    "        # Original VAE losses\n",
    "        # Recon loss\n",
    "        self._reconLoss = 1./2.*tf.norm(self.xRecon-self.x,ord=1,axis=1)\n",
    "        self._reconLossWeighted = tf.nn.softplus(self.q)*self._reconLoss\n",
    "        self.reconLossWeighted = tf.reduce_mean(self._reconLossWeighted)\n",
    "        # KL loss\n",
    "        self._klLoss = 0.5*tf.reduce_sum(tf.exp(self.zLogVarEncoded)+self.zMuEncoded**2-1.-self.zLogVarEncoded,1)\n",
    "        self._klLossWeighted = tf.nn.softplus(self.q)*self._klLoss\n",
    "        self.klLossWeighted = self.klWeight*tf.reduce_mean(self._klLossWeighted)\n",
    "        # Total loss\n",
    "        self.totalLoss = self.reconLossWeighted + self.klLossWeighted\n",
    "        # Solver\n",
    "        self.optm = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.lr,beta1=self.beta1,beta2=self.beta2,epsilon=self.epsilon)\\\n",
    "            .minimize(self.totalLoss)\n",
    "    # Check parameters\n",
    "    def _check_params(self):\n",
    "        _g_vars = tf.global_variables()\n",
    "        self.g_vars = [var for var in _g_vars if '%s/'%(self.name) in var.name]\n",
    "        if self.VERBOSE:\n",
    "            print (\"==== Global Variables ====\")\n",
    "        for i in range(len(self.g_vars)):\n",
    "            w_name  = self.g_vars[i].name\n",
    "            w_shape = self.g_vars[i].get_shape().as_list()\n",
    "            if self.VERBOSE:\n",
    "                print (\" [%02d] Name:[%s] Shape:[%s]\" % (i,w_name,w_shape))\n",
    "    # Train\n",
    "    def train(self,_sess,_X,_C,_Q,_maxIter,_batchSize,_PRINT_EVERY=100,_PLOT_EVERY=100):\n",
    "        # X: inputs [N x D]\n",
    "        # C: condition vectors [N x 1]\n",
    "        # Q: weighting vectors [N x 1]\n",
    "        self.sess = _sess\n",
    "        # Initialize variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        # Train\n",
    "        _Xtrain = _X\n",
    "        nX = _X.shape[0]\n",
    "        for _iter in range(_maxIter):\n",
    "            randIdx = np.random.permutation(nX)[:_batchSize] # Random indices every iteration\n",
    "            xBatch = _X[randIdx,:] # X batch\n",
    "            # Q batch\n",
    "            if _Q == None:\n",
    "                qBatch = np.ones(shape=(_batchSize))\n",
    "            else:\n",
    "                qBatch = _Q[randIdx]\n",
    "            if _C == None: # Original VAE (without conditioning)\n",
    "                feeds = {self.x:xBatch,self.q:qBatch,self.klWeight:1.0,self.isTraining:True,self.kp:0.9}\n",
    "            else: # Conditional VAE\n",
    "                cBatch = _C[randidx,:]\n",
    "                feeds = {self.x:xBatch,self.c:cBatch,self.q:qBatch,self.klWeight:1.0,self.isTraining:True,self.kp:0.9}\n",
    "            # Train\n",
    "            opers = [self.optm,self.totalLoss,self.reconLossWeighted,self.klLossWeighted]\n",
    "            _,totalLossVal,reconLossWeightedVal,klLossWeightedVal = self.sess.run(opers,feed_dict=feeds)\n",
    "            # Print \n",
    "            if (_iter%_PRINT_EVERY) == 0:\n",
    "                print (\"[%04d/%d] Loss: %.2f(recon:%.2f+kl:%.2f)\"%\n",
    "                       (_iter,_maxIter,totalLossVal,reconLossWeightedVal,klLossWeightedVal))\n",
    "            # Plot\n",
    "            if (_iter%_PLOT_EVERY) == 0:\n",
    "                nR,nC     = 1,10\n",
    "                zRandn = 1.*np.random.randn(nR*nC,self.zDim)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAE] xdim:[784] zdim:[10] hdim:[64, 64] cdim:[0]\n",
      "==== Global Variables ====\n",
      " [00] Name:[VAE/enc_lin0/weights:0] Shape:[[784, 64]]\n",
      " [01] Name:[VAE/enc_lin0/BatchNorm/beta:0] Shape:[[64]]\n",
      " [02] Name:[VAE/enc_lin0/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [03] Name:[VAE/enc_lin0/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [04] Name:[VAE/enc_lin1/weights:0] Shape:[[64, 64]]\n",
      " [05] Name:[VAE/enc_lin1/BatchNorm/beta:0] Shape:[[64]]\n",
      " [06] Name:[VAE/enc_lin1/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [07] Name:[VAE/enc_lin1/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [08] Name:[VAE/zMuEncoded/weights:0] Shape:[[64, 10]]\n",
      " [09] Name:[VAE/zMuEncoded/BatchNorm/beta:0] Shape:[[10]]\n",
      " [10] Name:[VAE/zMuEncoded/BatchNorm/moving_mean:0] Shape:[[10]]\n",
      " [11] Name:[VAE/zMuEncoded/BatchNorm/moving_variance:0] Shape:[[10]]\n",
      " [12] Name:[VAE/zLogVarEncoded/weights:0] Shape:[[64, 10]]\n",
      " [13] Name:[VAE/zLogVarEncoded/BatchNorm/beta:0] Shape:[[10]]\n",
      " [14] Name:[VAE/zLogVarEncoded/BatchNorm/moving_mean:0] Shape:[[10]]\n",
      " [15] Name:[VAE/zLogVarEncoded/BatchNorm/moving_variance:0] Shape:[[10]]\n",
      " [16] Name:[VAE/dec_lin0/weights:0] Shape:[[10, 64]]\n",
      " [17] Name:[VAE/dec_lin0/BatchNorm/beta:0] Shape:[[64]]\n",
      " [18] Name:[VAE/dec_lin0/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [19] Name:[VAE/dec_lin0/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [20] Name:[VAE/dec_lin1/weights:0] Shape:[[64, 64]]\n",
      " [21] Name:[VAE/dec_lin1/BatchNorm/beta:0] Shape:[[64]]\n",
      " [22] Name:[VAE/dec_lin1/BatchNorm/moving_mean:0] Shape:[[64]]\n",
      " [23] Name:[VAE/dec_lin1/BatchNorm/moving_variance:0] Shape:[[64]]\n",
      " [24] Name:[VAE/xRecon/weights:0] Shape:[[64, 784]]\n",
      " [25] Name:[VAE/xRecon/BatchNorm/beta:0] Shape:[[784]]\n",
      " [26] Name:[VAE/xRecon/BatchNorm/moving_mean:0] Shape:[[784]]\n",
      " [27] Name:[VAE/xRecon/BatchNorm/moving_variance:0] Shape:[[784]]\n",
      " [28] Name:[VAE/enc_lin0/weights/Adam:0] Shape:[[784, 64]]\n",
      " [29] Name:[VAE/enc_lin0/weights/Adam_1:0] Shape:[[784, 64]]\n",
      " [30] Name:[VAE/enc_lin0/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [31] Name:[VAE/enc_lin0/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [32] Name:[VAE/enc_lin1/weights/Adam:0] Shape:[[64, 64]]\n",
      " [33] Name:[VAE/enc_lin1/weights/Adam_1:0] Shape:[[64, 64]]\n",
      " [34] Name:[VAE/enc_lin1/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [35] Name:[VAE/enc_lin1/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [36] Name:[VAE/zMuEncoded/weights/Adam:0] Shape:[[64, 10]]\n",
      " [37] Name:[VAE/zMuEncoded/weights/Adam_1:0] Shape:[[64, 10]]\n",
      " [38] Name:[VAE/zMuEncoded/BatchNorm/beta/Adam:0] Shape:[[10]]\n",
      " [39] Name:[VAE/zMuEncoded/BatchNorm/beta/Adam_1:0] Shape:[[10]]\n",
      " [40] Name:[VAE/zLogVarEncoded/weights/Adam:0] Shape:[[64, 10]]\n",
      " [41] Name:[VAE/zLogVarEncoded/weights/Adam_1:0] Shape:[[64, 10]]\n",
      " [42] Name:[VAE/zLogVarEncoded/BatchNorm/beta/Adam:0] Shape:[[10]]\n",
      " [43] Name:[VAE/zLogVarEncoded/BatchNorm/beta/Adam_1:0] Shape:[[10]]\n",
      " [44] Name:[VAE/dec_lin0/weights/Adam:0] Shape:[[10, 64]]\n",
      " [45] Name:[VAE/dec_lin0/weights/Adam_1:0] Shape:[[10, 64]]\n",
      " [46] Name:[VAE/dec_lin0/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [47] Name:[VAE/dec_lin0/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [48] Name:[VAE/dec_lin1/weights/Adam:0] Shape:[[64, 64]]\n",
      " [49] Name:[VAE/dec_lin1/weights/Adam_1:0] Shape:[[64, 64]]\n",
      " [50] Name:[VAE/dec_lin1/BatchNorm/beta/Adam:0] Shape:[[64]]\n",
      " [51] Name:[VAE/dec_lin1/BatchNorm/beta/Adam_1:0] Shape:[[64]]\n",
      " [52] Name:[VAE/xRecon/weights/Adam:0] Shape:[[64, 784]]\n",
      " [53] Name:[VAE/xRecon/weights/Adam_1:0] Shape:[[64, 784]]\n",
      " [54] Name:[VAE/xRecon/BatchNorm/beta/Adam:0] Shape:[[784]]\n",
      " [55] Name:[VAE/xRecon/BatchNorm/beta/Adam_1:0] Shape:[[784]]\n",
      "[0/10000] Loss: 420.47(recon:409.45+lk:11.02)\n",
      "[100/10000] Loss: 218.63(recon:202.38+lk:16.25)\n",
      "[200/10000] Loss: 155.63(recon:135.07+lk:20.56)\n",
      "[300/10000] Loss: 162.05(recon:151.79+lk:10.26)\n",
      "[400/10000] Loss: 151.25(recon:140.77+lk:10.48)\n",
      "[500/10000] Loss: 146.20(recon:124.60+lk:21.60)\n",
      "[600/10000] Loss: 136.10(recon:124.65+lk:11.44)\n",
      "[700/10000] Loss: 163.53(recon:153.97+lk:9.56)\n",
      "[800/10000] Loss: 141.49(recon:131.87+lk:9.62)\n",
      "[900/10000] Loss: 133.75(recon:124.67+lk:9.08)\n",
      "[1000/10000] Loss: 125.78(recon:117.30+lk:8.48)\n",
      "[1100/10000] Loss: 137.39(recon:128.93+lk:8.46)\n",
      "[1200/10000] Loss: 135.27(recon:126.95+lk:8.32)\n",
      "[1300/10000] Loss: 118.50(recon:110.48+lk:8.02)\n",
      "[1400/10000] Loss: 130.73(recon:122.68+lk:8.05)\n",
      "[1500/10000] Loss: 115.12(recon:107.13+lk:7.99)\n",
      "[1600/10000] Loss: 111.90(recon:103.89+lk:8.01)\n",
      "[1700/10000] Loss: 114.09(recon:106.01+lk:8.08)\n",
      "[1800/10000] Loss: 120.50(recon:112.45+lk:8.05)\n",
      "[1900/10000] Loss: 119.55(recon:111.66+lk:7.89)\n",
      "[2000/10000] Loss: 110.81(recon:102.84+lk:7.97)\n",
      "[2100/10000] Loss: 133.54(recon:125.57+lk:7.98)\n",
      "[2200/10000] Loss: 109.00(recon:101.13+lk:7.88)\n",
      "[2300/10000] Loss: 108.96(recon:100.94+lk:8.02)\n",
      "[2400/10000] Loss: 106.83(recon:98.83+lk:8.00)\n",
      "[2500/10000] Loss: 105.22(recon:97.30+lk:7.92)\n",
      "[2600/10000] Loss: 124.20(recon:116.33+lk:7.87)\n",
      "[2700/10000] Loss: 109.38(recon:101.56+lk:7.82)\n",
      "[2800/10000] Loss: 135.43(recon:127.54+lk:7.89)\n",
      "[2900/10000] Loss: 108.98(recon:101.05+lk:7.93)\n",
      "[3000/10000] Loss: 104.96(recon:97.14+lk:7.81)\n",
      "[3100/10000] Loss: 125.03(recon:117.22+lk:7.82)\n",
      "[3200/10000] Loss: 111.48(recon:103.71+lk:7.76)\n",
      "[3300/10000] Loss: 117.95(recon:110.07+lk:7.88)\n",
      "[3400/10000] Loss: 105.11(recon:97.35+lk:7.76)\n",
      "[3500/10000] Loss: 115.30(recon:107.46+lk:7.84)\n",
      "[3600/10000] Loss: 101.45(recon:93.68+lk:7.76)\n",
      "[3700/10000] Loss: 104.46(recon:96.66+lk:7.80)\n",
      "[3800/10000] Loss: 110.65(recon:102.78+lk:7.88)\n",
      "[3900/10000] Loss: 103.42(recon:95.71+lk:7.71)\n",
      "[4000/10000] Loss: 102.51(recon:94.63+lk:7.88)\n",
      "[4100/10000] Loss: 89.64(recon:81.63+lk:8.01)\n",
      "[4200/10000] Loss: 74.36(recon:66.29+lk:8.07)\n",
      "[4300/10000] Loss: 70.71(recon:62.55+lk:8.16)\n",
      "[4400/10000] Loss: 69.34(recon:61.54+lk:7.80)\n",
      "[4500/10000] Loss: 74.26(recon:66.50+lk:7.76)\n",
      "[4600/10000] Loss: 70.51(recon:62.76+lk:7.74)\n",
      "[4700/10000] Loss: 71.83(recon:64.18+lk:7.65)\n",
      "[4800/10000] Loss: 70.52(recon:62.86+lk:7.66)\n",
      "[4900/10000] Loss: 69.82(recon:62.26+lk:7.56)\n",
      "[5000/10000] Loss: 72.78(recon:65.17+lk:7.60)\n",
      "[5100/10000] Loss: 71.31(recon:63.68+lk:7.64)\n",
      "[5200/10000] Loss: 71.58(recon:64.09+lk:7.49)\n",
      "[5300/10000] Loss: 72.26(recon:64.59+lk:7.67)\n",
      "[5400/10000] Loss: 71.44(recon:63.93+lk:7.50)\n",
      "[5500/10000] Loss: 72.95(recon:65.42+lk:7.53)\n",
      "[5600/10000] Loss: 71.63(recon:64.03+lk:7.60)\n",
      "[5700/10000] Loss: 69.53(recon:62.13+lk:7.40)\n",
      "[5800/10000] Loss: 72.06(recon:64.57+lk:7.49)\n",
      "[5900/10000] Loss: 72.90(recon:65.51+lk:7.39)\n",
      "[6000/10000] Loss: 68.30(recon:60.89+lk:7.41)\n",
      "[6100/10000] Loss: 74.18(recon:66.73+lk:7.44)\n",
      "[6200/10000] Loss: 72.35(recon:64.91+lk:7.43)\n",
      "[6300/10000] Loss: 72.37(recon:65.01+lk:7.35)\n",
      "[6400/10000] Loss: 69.77(recon:62.38+lk:7.39)\n",
      "[6500/10000] Loss: 71.10(recon:63.62+lk:7.48)\n",
      "[6600/10000] Loss: 70.91(recon:63.66+lk:7.25)\n",
      "[6700/10000] Loss: 72.72(recon:65.29+lk:7.42)\n",
      "[6800/10000] Loss: 72.05(recon:64.61+lk:7.44)\n",
      "[6900/10000] Loss: 73.52(recon:66.25+lk:7.27)\n",
      "[7000/10000] Loss: 71.94(recon:64.50+lk:7.44)\n",
      "[7100/10000] Loss: 73.33(recon:65.94+lk:7.39)\n",
      "[7200/10000] Loss: 69.62(recon:62.34+lk:7.27)\n",
      "[7300/10000] Loss: 72.30(recon:64.98+lk:7.33)\n",
      "[7400/10000] Loss: 69.73(recon:62.47+lk:7.26)\n",
      "[7500/10000] Loss: 73.97(recon:66.57+lk:7.39)\n",
      "[7600/10000] Loss: 71.66(recon:64.19+lk:7.46)\n",
      "[7700/10000] Loss: 70.49(recon:63.21+lk:7.28)\n",
      "[7800/10000] Loss: 72.13(recon:64.75+lk:7.38)\n",
      "[7900/10000] Loss: 72.55(recon:65.24+lk:7.31)\n",
      "[8000/10000] Loss: 73.36(recon:66.05+lk:7.31)\n",
      "[8100/10000] Loss: 70.01(recon:62.72+lk:7.29)\n",
      "[8200/10000] Loss: 71.70(recon:64.33+lk:7.37)\n",
      "[8300/10000] Loss: 70.64(recon:63.38+lk:7.26)\n",
      "[8400/10000] Loss: 71.31(recon:64.02+lk:7.28)\n",
      "[8500/10000] Loss: 71.75(recon:64.45+lk:7.30)\n",
      "[8600/10000] Loss: 69.70(recon:62.57+lk:7.13)\n",
      "[8700/10000] Loss: 70.19(recon:62.92+lk:7.27)\n",
      "[8800/10000] Loss: 71.92(recon:64.52+lk:7.39)\n",
      "[8900/10000] Loss: 71.86(recon:64.58+lk:7.29)\n",
      "[9000/10000] Loss: 68.49(recon:61.22+lk:7.27)\n",
      "[9100/10000] Loss: 72.10(recon:64.82+lk:7.29)\n",
      "[9200/10000] Loss: 72.40(recon:65.13+lk:7.27)\n",
      "[9300/10000] Loss: 72.32(recon:65.22+lk:7.10)\n",
      "[9400/10000] Loss: 73.55(recon:66.42+lk:7.13)\n",
      "[9500/10000] Loss: 70.09(recon:62.89+lk:7.19)\n",
      "[9600/10000] Loss: 70.78(recon:63.50+lk:7.28)\n",
      "[9700/10000] Loss: 73.40(recon:66.23+lk:7.18)\n",
      "[9800/10000] Loss: 73.25(recon:66.02+lk:7.24)\n",
      "[9900/10000] Loss: 71.04(recon:63.89+lk:7.15)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(0); np.random.seed(0)\n",
    "V = vae_class(_name='VAE',_xDim=784,_zDim=10,_hDims=[64,64],_cDim=0, \n",
    "             _actv=tf.nn.relu,_bn=slim.batch_norm,\n",
    "             _lr=0.001,_beta1=0.9,_beta2=0.9,_epsilon=0.1,\n",
    "             _VERBOSE=True)\n",
    "sess = gpu_sess()\n",
    "# We will use MNIST\n",
    "X = mnist.train.images\n",
    "Y = mnist.train.labels\n",
    "V.train(_sess=sess,_X=X,_C=None,_Q=None,_maxIter=(int)(1e4),_batchSize=256,_PRINT_EVERY=100,_PLOT_EVERY=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
