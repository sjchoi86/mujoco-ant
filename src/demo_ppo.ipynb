{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PPO on Ant from [here](https://github.com/pat-coady/trpo)\n",
    "#### More descriptions can be found on Patrick Coady's blog regarding [gym and ppo](https://learningai.io/projects/2017/07/28/ai-gym-workout.html) or [descriptions about Ant env](https://gist.github.com/pat-coady/bac60888f011199aad72d2f1e6f5a4fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Loaded\n"
     ]
    }
   ],
   "source": [
    "import gym,mujoco_py,warnings,time,os,glob,shutil,csv,skvideo.io\n",
    "gym.logger.set_level(40)\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs import mujoco\n",
    "from datetime import datetime\n",
    "from util import PID_class,Scaler,Logger,display_frames_as_gif\n",
    "from custom_ant import AntEnvCustom\n",
    "from ppo import NNValueFunction,Policy,run_episode,run_policy,add_value,discount,\\\n",
    "    add_disc_sum_rew,add_gae,build_train_set,log_batch_stats,run_episode_vid\n",
    "np.set_printoptions(precision=2,linewidth=150)\n",
    "%matplotlib inline  \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "print (\"Packages Loaded\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Ant Environment made by SJ.\n",
      "obs_dim:[111] act_dim:[8]\n",
      "Value Params -- h1: 1120, h2: 74, h3: 5, lr: 0.00116\n",
      "Policy Params -- h1: 1120, h2: 299, h3: 80, lr: 5.2e-05, logvar_speed: 16\n",
      "setting up loss with KL penalty\n"
     ]
    }
   ],
   "source": [
    "env = AntEnvCustom()\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "env.reset() # Reset \n",
    "# render_img = env.render(mode='rgb_array')\n",
    "print (\"obs_dim:[%d] act_dim:[%d]\"%(obs_dim,act_dim))\n",
    "\n",
    "obs_dim += 1  # add 1 to obs dimension for time step feature (see run_episode())\n",
    "# Logger\n",
    "env_name = 'Ant'\n",
    "now = datetime.utcnow().strftime(\"%b-%d_%H:%M:%S\")  # create unique directories\n",
    "logger = Logger(logName=env_name,now=now,_NOTUSE=True)\n",
    "aigym_path = os.path.join('/tmp', env_name, now)\n",
    "# Scaler\n",
    "scaler = Scaler(obs_dim)\n",
    "# Value function\n",
    "hid1_mult = 10\n",
    "val_func = NNValueFunction(obs_dim, hid1_mult)\n",
    "# Policy Function\n",
    "kl_targ = 0.003\n",
    "policy_logvar = -1.0\n",
    "policy = Policy(obs_dim, act_dim, kl_targ, hid1_mult, policy_logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run policy for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observes shape: (27, 112)\n",
      "actions shape: (27, 8)\n",
      "rewards shape: (27,)\n",
      "unscaled_obs shape: (27, 112)\n",
      "values shape: (27,)\n",
      "disc_sum_rew shape: (27,)\n",
      "advantages shape: (27,)\n"
     ]
    }
   ],
   "source": [
    "trajectories = run_policy(env, policy, scaler, logger, episodes=5)\n",
    "add_value(trajectories, val_func)  # add estimated values to episodes\n",
    "gamma = 0.995 # Discount factor \n",
    "lam = 0.95 # Lambda for GAE\n",
    "add_disc_sum_rew(trajectories, gamma)  # calculated discounted sum of Rs\n",
    "add_gae(trajectories, gamma, lam)  # calculate advantage\n",
    "print ('observes shape:',trajectories[0]['observes'].shape)\n",
    "print ('actions shape:',trajectories[0]['actions'].shape)\n",
    "print ('rewards shape:',trajectories[0]['rewards'].shape)\n",
    "print ('unscaled_obs shape:',trajectories[0]['unscaled_obs'].shape)\n",
    "print ('values shape:',trajectories[0]['values'].shape)\n",
    "print ('disc_sum_rew shape:',trajectories[0]['disc_sum_rew'].shape)\n",
    "print ('advantages shape:',trajectories[0]['advantages'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_VID = True\n",
    "MAKE_GIF = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000](#total:50) sumRwd:[-10673.377](cntct:-5.155+ctrl:-9244.300+fwd:12.265+head:-6274.187+srv:4838.000)\n",
      "Creating window glfw\n",
      "  [^] sumRwd:[-8.423] Xdisp:[-0.034] hDisp:[5.3]\n",
      "[vids/ant_ppo_epoch000.mp4] saved.\n",
      "[1/10000](#total:100) sumRwd:[-8664.426](cntct:-4.282+ctrl:-7905.050+fwd:-188.290+head:-4803.805+srv:4237.000)\n",
      "[2/10000](#total:150) sumRwd:[-7590.451](cntct:-3.927+ctrl:-6976.492+fwd:-120.645+head:-4149.386+srv:3660.000)\n",
      "[3/10000](#total:200) sumRwd:[-12299.852](cntct:-5.238+ctrl:-9772.762+fwd:-250.157+head:-7309.695+srv:5038.000)\n",
      "[4/10000](#total:250) sumRwd:[-9932.283](cntct:-4.735+ctrl:-8499.111+fwd:40.085+head:-5941.521+srv:4473.000)\n",
      "[5/10000](#total:300) sumRwd:[-9368.434](cntct:-4.285+ctrl:-7767.812+fwd:182.308+head:-5866.645+srv:4088.000)\n",
      "[6/10000](#total:350) sumRwd:[-7363.795](cntct:-4.047+ctrl:-7530.047+fwd:-110.979+head:-3620.723+srv:3902.000)\n",
      "[7/10000](#total:400) sumRwd:[-10128.814](cntct:-4.892+ctrl:-8762.523+fwd:-76.960+head:-5821.439+srv:4537.000)\n",
      "[8/10000](#total:450) sumRwd:[-7319.529](cntct:-3.715+ctrl:-6584.249+fwd:-203.247+head:-3943.318+srv:3415.000)\n",
      "[9/10000](#total:500) sumRwd:[-6873.910](cntct:-3.755+ctrl:-7237.267+fwd:12.977+head:-3374.865+srv:3729.000)\n",
      "[10/10000](#total:550) sumRwd:[-8510.424](cntct:-4.114+ctrl:-7663.939+fwd:-72.644+head:-4676.727+srv:3907.000)\n",
      "[11/10000](#total:600) sumRwd:[-7441.784](cntct:-3.468+ctrl:-6530.818+fwd:-167.556+head:-4092.943+srv:3353.000)\n",
      "[12/10000](#total:650) sumRwd:[-8624.566](cntct:-4.212+ctrl:-7497.184+fwd:-30.003+head:-5024.167+srv:3931.000)\n",
      "[13/10000](#total:700) sumRwd:[-9196.805](cntct:-4.383+ctrl:-8031.107+fwd:-242.754+head:-4984.560+srv:4066.000)\n",
      "[14/10000](#total:750) sumRwd:[-10590.787](cntct:-4.872+ctrl:-9291.602+fwd:-258.107+head:-5683.206+srv:4647.000)\n",
      "[15/10000](#total:800) sumRwd:[-8131.551](cntct:-4.136+ctrl:-7725.840+fwd:55.165+head:-4312.740+srv:3856.000)\n",
      "[16/10000](#total:850) sumRwd:[-8813.091](cntct:-4.455+ctrl:-8921.842+fwd:-115.766+head:-4144.027+srv:4373.000)\n",
      "[17/10000](#total:900) sumRwd:[-8487.740](cntct:-4.328+ctrl:-8483.194+fwd:-162.168+head:-4017.049+srv:4179.000)\n",
      "[18/10000](#total:950) sumRwd:[-8287.504](cntct:-3.947+ctrl:-7817.186+fwd:-29.476+head:-4263.894+srv:3827.000)\n",
      "[19/10000](#total:1000) sumRwd:[-10631.304](cntct:-4.578+ctrl:-9130.200+fwd:234.289+head:-6277.815+srv:4547.000)\n"
     ]
    }
   ],
   "source": [
    "maxEpoch  = 10000\n",
    "batchSize = 50\n",
    "for _epoch in range(maxEpoch):\n",
    "    # 1. Run policy\n",
    "    trajectories = run_policy(env, policy, scaler, logger, episodes=batchSize)\n",
    "    # 2. Get (predict) value from the critic network \n",
    "    add_value(trajectories, val_func)  # add estimated values to episodes\n",
    "    # 3. Get GAE\n",
    "    gamma = 0.995 # Discount factor \n",
    "    lam = 0.95 # Lambda for GAE\n",
    "    add_disc_sum_rew(trajectories, gamma)  # calculated discounted sum of Rs\n",
    "    add_gae(trajectories, gamma, lam)  # calculate advantage\n",
    "    # concatenate all episodes into single NumPy arrays\n",
    "    observes, actions, advantages, disc_sum_rew = build_train_set(trajectories)\n",
    "    # add various stats to training log:\n",
    "    # log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode)\n",
    "    # Update\n",
    "    policy.update(observes, actions, advantages, logger)  # update policy\n",
    "    val_func.fit(observes, disc_sum_rew, logger)  # update value function\n",
    "    # logger.write(display=True)  # write logger results to file and stdout\n",
    "    \n",
    "    # Print\n",
    "    for _tIdx in range(len(trajectories)):\n",
    "        rs = trajectories[_tIdx]['rewards']\n",
    "        if _tIdx == 0: rTotal = rs\n",
    "        else: rTotal = np.concatenate((rTotal,rs))\n",
    "        # Reward details      \n",
    "    sumRwd = rTotal.sum()\n",
    "    reward_contacts,reward_ctrls,reward_forwards,reward_headings,reward_survives = [],[],[],[],[]\n",
    "    for traj in trajectories:\n",
    "        cTraj = traj['rDetails']\n",
    "        for _iIdx in range(len(cTraj)):\n",
    "            reward_contacts.append(cTraj[_iIdx]['reward_contact'])\n",
    "            reward_ctrls.append(cTraj[_iIdx]['reward_ctrl'])\n",
    "            reward_forwards.append(cTraj[_iIdx]['reward_forward'])\n",
    "            reward_headings.append(cTraj[_iIdx]['reward_heading'])\n",
    "            reward_survives.append(cTraj[_iIdx]['reward_survive'])\n",
    "    sumReward_contact = np.asarray(reward_contacts).sum()\n",
    "    sumReward_ctrl = np.asarray(reward_ctrls).sum()\n",
    "    sumReward_forward = np.asarray(reward_forwards).sum()\n",
    "    sumReward_heading = np.asarray(reward_headings).sum()\n",
    "    sumReward_survive = np.asarray(reward_survives).sum()\n",
    "    print (\"[%d/%d](#total:%d) sumRwd:[%.3f](cntct:%.3f+ctrl:%.3f+fwd:%.3f+head:%.3f+srv:%.3f)\"%\n",
    "           (_epoch,maxEpoch,(_epoch+1)*batchSize,sumRwd,\n",
    "           sumReward_contact,sumReward_ctrl,sumReward_forward,sumReward_heading,sumReward_survive))\n",
    "    \n",
    "    # SHOW EVERY \n",
    "    PLOT_EVERY = 20 \n",
    "    DO_ANIMATE = False\n",
    "    if ((_epoch%PLOT_EVERY)==0 ) | (_epoch==(maxEpoch-1)):\n",
    "        ret = run_episode_vid(env, policy, scaler)\n",
    "        print (\"  [^] sumRwd:[%.3f] Xdisp:[%.3f] hDisp:[%.1f]\"%\n",
    "               (np.asarray(ret['rewards']).sum(),ret['xDisp'],ret['hDisp']))\n",
    "        if MAKE_GIF:\n",
    "            display_frames_as_gif(ret['frames'])\n",
    "        if SAVE_VID:\n",
    "            outputdata = np.asarray(ret['frames']).astype(np.uint8)\n",
    "            vidName = 'vids/ant_ppo_epoch%03d.mp4'%(_epoch)\n",
    "            skvideo.io.vwrite(vidName,outputdata)\n",
    "            print (\"[%s] saved.\"%(vidName))\n",
    "print (\"Done.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animate final motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_VID_FINAL = True\n",
    "MAKE_GIF_FINAL = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _i in range(3):\n",
    "    ret = run_episode_vid(env, policy, scaler)\n",
    "    if MAKE_GIF_FINAL:\n",
    "        display_frames_as_gif(ret['frames'])\n",
    "    if SAVE_VID_FINAL:\n",
    "        outputdata = np.asarray(ret['frames']).astype(np.uint8)\n",
    "        vidName = 'vids/ant_ppo_final_%d.mp4'%(_i)\n",
    "        skvideo.io.vwrite(vidName,outputdata)\n",
    "        print (\"[%s] saved.\"%(vidName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_CLOSE = False # There is no turning back. \n",
    "if DO_CLOSE:\n",
    "    logger.close()\n",
    "    policy.close_sess()\n",
    "    val_func.close_sess()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
