{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PPO on Ant from [here](https://github.com/pat-coady/trpo)\n",
    "#### More descriptions can be found on Patrick Coady's blog regarding [gym and ppo](https://learningai.io/projects/2017/07/28/ai-gym-workout.html) or [descriptions about Ant env](https://gist.github.com/pat-coady/bac60888f011199aad72d2f1e6f5a4fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Loaded\n"
     ]
    }
   ],
   "source": [
    "import gym,mujoco_py,warnings,time,os,glob,shutil,csv,skvideo.io\n",
    "gym.logger.set_level(40)\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs import mujoco\n",
    "from datetime import datetime\n",
    "from util import PID_class,Scaler,Logger,display_frames_as_gif\n",
    "from custom_ant import AntEnvCustom\n",
    "from ppo import NNValueFunction,Policy,run_episode,run_policy,add_value,discount,\\\n",
    "    add_disc_sum_rew,add_gae,build_train_set,log_batch_stats,run_episode_vid\n",
    "np.set_printoptions(precision=2,linewidth=150)\n",
    "%matplotlib inline  \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "print (\"Packages Loaded\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Ant Environment made by SJ.\n",
      "obs_dim:[111] act_dim:[8]\n",
      "Value Params -- h1: 1120, h2: 74, h3: 5, lr: 0.00116\n",
      "Policy Params -- h1: 1120, h2: 299, h3: 80, lr: 5.2e-05, logvar_speed: 16\n",
      "setting up loss with KL penalty\n"
     ]
    }
   ],
   "source": [
    "env = AntEnvCustom()\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "env.reset() # Reset \n",
    "# render_img = env.render(mode='rgb_array')\n",
    "print (\"obs_dim:[%d] act_dim:[%d]\"%(obs_dim,act_dim))\n",
    "\n",
    "obs_dim += 1  # add 1 to obs dimension for time step feature (see run_episode())\n",
    "# Logger\n",
    "env_name = 'Ant'\n",
    "now = datetime.utcnow().strftime(\"%b-%d_%H:%M:%S\")  # create unique directories\n",
    "logger = Logger(logName=env_name,now=now,_NOTUSE=True)\n",
    "aigym_path = os.path.join('/tmp', env_name, now)\n",
    "# Scaler\n",
    "scaler = Scaler(obs_dim)\n",
    "# Value function\n",
    "hid1_mult = 10\n",
    "val_func = NNValueFunction(obs_dim, hid1_mult)\n",
    "# Policy Function\n",
    "kl_targ = 0.003\n",
    "policy_logvar = -1.0\n",
    "policy = Policy(obs_dim, act_dim, kl_targ, hid1_mult, policy_logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run policy for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observes shape: (42, 112)\n",
      "actions shape: (42, 8)\n",
      "rewards shape: (42,)\n",
      "unscaled_obs shape: (42, 112)\n",
      "values shape: (42,)\n",
      "disc_sum_rew shape: (42,)\n",
      "advantages shape: (42,)\n"
     ]
    }
   ],
   "source": [
    "trajectories = run_policy(env, policy, scaler, logger, episodes=5)\n",
    "add_value(trajectories, val_func)  # add estimated values to episodes\n",
    "gamma = 0.995 # Discount factor \n",
    "lam = 0.95 # Lambda for GAE\n",
    "add_disc_sum_rew(trajectories, gamma)  # calculated discounted sum of Rs\n",
    "add_gae(trajectories, gamma, lam)  # calculate advantage\n",
    "print ('observes shape:',trajectories[0]['observes'].shape)\n",
    "print ('actions shape:',trajectories[0]['actions'].shape)\n",
    "print ('rewards shape:',trajectories[0]['rewards'].shape)\n",
    "print ('unscaled_obs shape:',trajectories[0]['unscaled_obs'].shape)\n",
    "print ('values shape:',trajectories[0]['values'].shape)\n",
    "print ('disc_sum_rew shape:',trajectories[0]['disc_sum_rew'].shape)\n",
    "print ('advantages shape:',trajectories[0]['advantages'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_VID = True\n",
    "MAKE_GIF = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000](#total:50) sumRwd:[-129.968](cntct:-0.083+ctrl:-149.131+fwd:-9.885+head:-47.769+srv:76.900) tickAvg:[76]\n",
      "Creating window glfw\n",
      "  [^] sumRwd:[-33.288] Xdisp:[-0.314] hDisp:[-67.1]\n",
      "[vids/ant_ppo_epoch000.mp4] saved.\n",
      "[1/10000](#total:100) sumRwd:[-138.597](cntct:-0.093+ctrl:-167.291+fwd:-7.121+head:-51.072+srv:86.980) tickAvg:[86]\n",
      "[2/10000](#total:150) sumRwd:[-151.172](cntct:-0.090+ctrl:-165.621+fwd:-7.401+head:-63.540+srv:85.480) tickAvg:[85]\n",
      "[3/10000](#total:200) sumRwd:[-130.932](cntct:-0.085+ctrl:-151.000+fwd:-11.546+head:-46.801+srv:78.500) tickAvg:[78]\n",
      "[4/10000](#total:250) sumRwd:[-176.840](cntct:-0.100+ctrl:-186.039+fwd:-12.293+head:-72.548+srv:94.140) tickAvg:[94]\n",
      "[5/10000](#total:300) sumRwd:[-117.027](cntct:-0.081+ctrl:-149.985+fwd:-1.702+head:-42.759+srv:77.500) tickAvg:[77]\n",
      "[6/10000](#total:350) sumRwd:[-131.626](cntct:-0.081+ctrl:-151.955+fwd:-5.846+head:-51.524+srv:77.780) tickAvg:[77]\n"
     ]
    }
   ],
   "source": [
    "maxEpoch  = 10000\n",
    "batchSize = 50\n",
    "for _epoch in range(maxEpoch):\n",
    "    # 1. Run policy\n",
    "    trajectories = run_policy(env, policy, scaler, logger, episodes=batchSize)\n",
    "    # 2. Get (predict) value from the critic network \n",
    "    add_value(trajectories, val_func)  # add estimated values to episodes\n",
    "    # 3. Get GAE\n",
    "    gamma = 0.995 # Discount factor \n",
    "    lam = 0.95 # Lambda for GAE\n",
    "    add_disc_sum_rew(trajectories, gamma)  # calculated discounted sum of Rs\n",
    "    add_gae(trajectories, gamma, lam)  # calculate advantage\n",
    "    # concatenate all episodes into single NumPy arrays\n",
    "    observes, actions, advantages, disc_sum_rew = build_train_set(trajectories)\n",
    "    # add various stats to training log:\n",
    "    # log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode)\n",
    "    # Update\n",
    "    policy.update(observes, actions, advantages, logger)  # update policy\n",
    "    val_func.fit(observes, disc_sum_rew, logger)  # update value function\n",
    "    # logger.write(display=True)  # write logger results to file and stdout\n",
    "    \n",
    "    # Print\n",
    "    for _tIdx in range(len(trajectories)):\n",
    "        rs = trajectories[_tIdx]['rewards']\n",
    "        if _tIdx == 0: rTotal = rs\n",
    "        else: rTotal = np.concatenate((rTotal,rs))\n",
    "        # Reward details      \n",
    "    reward_contacts,reward_ctrls,reward_forwards,reward_headings,reward_survives = [],[],[],[],[]\n",
    "    tickSum = 0\n",
    "    for traj in trajectories:\n",
    "        tickSum += traj['rewards'].shape[0]\n",
    "        cTraj = traj['rDetails']\n",
    "        for _iIdx in range(len(cTraj)):\n",
    "            reward_contacts.append(cTraj[_iIdx]['reward_contact'])\n",
    "            reward_ctrls.append(cTraj[_iIdx]['reward_ctrl'])\n",
    "            reward_forwards.append(cTraj[_iIdx]['reward_forward'])\n",
    "            reward_headings.append(cTraj[_iIdx]['reward_heading'])\n",
    "            reward_survives.append(cTraj[_iIdx]['reward_survive'])\n",
    "    tickAvg = tickSum / batchSize\n",
    "    sumRwd = rTotal.sum() / batchSize\n",
    "    sumReward_contact = np.asarray(reward_contacts).sum() / batchSize\n",
    "    sumReward_ctrl = np.asarray(reward_ctrls).sum() / batchSize\n",
    "    sumReward_forward = np.asarray(reward_forwards).sum() / batchSize\n",
    "    sumReward_heading = np.asarray(reward_headings).sum() / batchSize\n",
    "    sumReward_survive = np.asarray(reward_survives).sum() / batchSize\n",
    "    print (\"[%d/%d](#total:%d) sumRwd:[%.3f](cntct:%.3f+ctrl:%.3f+fwd:%.3f+head:%.3f+srv:%.3f) tickAvg:[%d]\"%\n",
    "           (_epoch,maxEpoch,(_epoch+1)*batchSize,sumRwd,\n",
    "           sumReward_contact,sumReward_ctrl,sumReward_forward,sumReward_heading,sumReward_survive,tickAvg))\n",
    "    \n",
    "    # SHOW EVERY \n",
    "    PLOT_EVERY = 20 \n",
    "    DO_ANIMATE = False\n",
    "    if ((_epoch%PLOT_EVERY)==0 ) | (_epoch==(maxEpoch-1)):\n",
    "        ret = run_episode_vid(env, policy, scaler)\n",
    "        print (\"  [^] sumRwd:[%.3f] Xdisp:[%.3f] hDisp:[%.1f]\"%\n",
    "               (np.asarray(ret['rewards']).sum(),ret['xDisp'],ret['hDisp']))\n",
    "        if MAKE_GIF:\n",
    "            display_frames_as_gif(ret['frames'])\n",
    "        if SAVE_VID:\n",
    "            outputdata = np.asarray(ret['frames']).astype(np.uint8)\n",
    "            vidName = 'vids/ant_ppo_epoch%03d.mp4'%(_epoch)\n",
    "            skvideo.io.vwrite(vidName,outputdata)\n",
    "            print (\"[%s] saved.\"%(vidName))\n",
    "print (\"Done.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animate final motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_VID_FINAL = True\n",
    "MAKE_GIF_FINAL = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _i in range(3):\n",
    "    ret = run_episode_vid(env, policy, scaler)\n",
    "    if MAKE_GIF_FINAL:\n",
    "        display_frames_as_gif(ret['frames'])\n",
    "    if SAVE_VID_FINAL:\n",
    "        outputdata = np.asarray(ret['frames']).astype(np.uint8)\n",
    "        vidName = 'vids/ant_ppo_final_%d.mp4'%(_i)\n",
    "        skvideo.io.vwrite(vidName,outputdata)\n",
    "        print (\"[%s] saved.\"%(vidName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_CLOSE = False # There is no turning back. \n",
    "if DO_CLOSE:\n",
    "    logger.close()\n",
    "    policy.close_sess()\n",
    "    val_func.close_sess()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
